---
title: "Boosted Trees"
subtitle: "Project 2, by Group 2"
author: "CMDA 4654"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        show.signif.stars = FALSE)
```

# Outline

- Summary of boosted trees
- Theory behind gradient boosting and boosted trees
- Example where other methods are not effective
- Example using boosted tree classification
- Example on a large dataset

---

# Summary of Boosted Trees

Commonly referred too as gradient boosting, boosted trees is a method that works against the common problem among trees - overfitting a dataset. It's called "gradient" boosting because it uses gradient descent to iteratively find the best fit to the data. The idea stems from a method called Adaboost, which we will not discuss here but you can find plenty of resources online. Understanding Adaboost really helps you understand the underlying methodology behind gradient boosting.

There are two types of gradient tree boosting, **regression** and **classification**. The concept is similar but the implementation is different depending on the use case. In this presentation we will touch on classification but focus on regression.

---

# Summary of Boosted Trees

Gradient boost for regression works by creating small regression trees (known as weak learners), calculating the errors from that weak learner, and fitting another tree to predict those errors. These models are then combined to form the gradient boosted model.

Each step of the algorithm creates a new tree that may be small or large using random samples of columns *and* rows, depending on your tuning parameters. All of the trees are then **scaled** by a tuning parameter between 0 and 1. *This scale tuning parameter basically determines how quickly our boosted trees find a good fit.*

This happens for either **A)** a specified number of steps, or **B)** the errors become sufficiently small.

You can see how this is similar to random forests in that the algorithm creates many trees, and can use random samples of columns. However, the algorithm can also use samples of rows, and it does not *have* to use random samples of either but can also use the entire dataset.

---

# Boosted Regression
## References
- Gradient Boosting Explained: http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/
- Gradient Boosting: https://en.wikipedia.org/wiki/Gradient_boosting
- StatQuest - Gradient Boost, Parts 1 & 2: https://www.youtube.com/watch?v=3CC4N4z3GJc&t=582s

---

# Boosted Regression - Main Idea

The main idea behind boosted regression is that we fit a model, then fit another model to the residuals, and then the combination of these becomes the new model. We iterate this idea many times to find the optimal model. In pseudocode:

**Step 1:** We fit the original model.

$$F_1(x) = y$$

**Step 2:** We fit a new model to the residuals of the model in Step 1.

$$h_1(x) = y - F_1(x)$$

**Step 3:** We combine the two previous models to find the next step in the algorithm.

$$F_2(x) = F_1(x) + h_1(x)$$

---

# Boosted Regression - Step 1

To start the regression algorithm, we initialize the model with the average of our $y$ values. This becomes our starting point because initially we want to minimize the squared error. 

$$F_0(x) = \underset{\gamma}{argmin} \sum_{i=1}^n{L(y_i, \gamma)} = \underset{\gamma}{argmin} \sum_{i=1}^n{(\gamma - y_i)^2} = \frac{1}{n}\sum_{i=1}^n{y_i}$$

Now, we can define the following models just like we did before in pseudocode.

$$F_{m+1}(x) = F_m(x) + h_m(x)=y,\ for\ m\ge0$$

where $h_m$ is a base learner (e.g. weak learner; tree).

**Note:** How do we select $M$? This value determines how many times we iterate until we find the best model. This is best done by cross-validation. We'll discuss this later in the regularization section.

---

# Boosted Regression - Step 2

After we initialize the model, $F_0(x)$, we can begin iterating. Each iteration contains 4 steps:

**Step 1:** Compute the pseudo-residuals (this is the gradient descent part of *gradient* boost).
$$r_{im} = - \left[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)}\ for\ i=1,...,n$$

**Step 2:** Fit a base learner (weak learner, e.g. a tree) to the pseudo-residuals. This is $h_m(x)$.

**Step 3:** Compute the step magnitude multiplier, $\gamma_m$, by solving the 1D optimization problem:
$$\gamma_m = \underset{\gamma}{argmin} \sum_{i=1}^n{L(y_i,\ F_{m-1}(x_i) + \gamma h_m(x_i))}$$

**Step 4:** Update the model with the new pieces:
$$F_m(x) = F_{m-1}(x)+\gamma_mh_m(x)$$

---

# Boosted Regression - Step 3

After we compute all the iterations of the algorithm, our final model is then:
$$for\ m=1\ to\ M,\\ F_M(x)$$

with inputs:
1. Training data set, ${(x_i, y_i)}_{i=1}^{n}$
2. Loss function, $L(y, F(x))$
3. Number of iterations, $M$

We will talk about $M$ and a few more paramaters of the algorithm next.

---

# Boosted Trees - Regularization

One of the main goals in using boosted trees is to avoid overfitting the data. To achieve a boosted model that is well-generalized and not overfitted, we can make use of **regularization**. These are just a few of the regularization parameters that we can use to control effectiveness of the boosting algorithm:
- The number of iterations, $M$
- Maximum tree depth
- Minimum number of observations in tree leaves
- Penalizing complexity: pruning back the weak learners that don't provide any benefit
- Shrinkage parameter, $\nu$, known as the *"learning rate"*
- Size of the trees

**The number of iterations, $M$**

This is the number of times we should run the gradient descent algorithm (basically the number of trees we create). As stated previously, $M$ is most commonly chosen using cross-validation. Increasing $M$ reduces error on the training set, but making $M$ too high can lead to overfitting.

---

# BT - Regularization Parameters

**Maximum tree depth**

This is the maximum distance from the root node to the furthest leaf node in each of our trees. This helps control how "weak" our weak learners should actually be.

**Minimum number of observations in tree leaves**

This parameter controls the minimum number of observations within each leaf of the weak learners. This tells the weak learners to basically ignore any splits or new terminal nodes that have fewer observations than this number. This helps to reduce variance in the predictions at the leaves.

**Penalizing complexity: pruning back the weak learners that don't provide any benefit**

This is defined as the "proportional number of leaves in the learned trees." We can employ a post-pruning algorithm that removes branches failing to reduce the loss by a certain threshold. This basically helps to keep the trees from becoming too strong and overfitting.

---

# BT - Regularization Parameters, cont.

**Shrinkage parameter, $\nu$, known as the "learning rate"**

This is arguably one of the most important parameters. This parameter is a value between 0 and 1, and controls how fast or slow our boosted algorithm finds the optimal model. A lower learning rate (e.g. 0.1) means higher computation time and more iterations, but a better and more generalized model. It is applied to the algorithm as:
$$F_m(x) = F_{m-1}(x)+\nu\cdot\gamma_mh_m(x)$$

**Size of the trees**

The tree size parameter, $J$, controls the number of terminal nodes in each weak learner. $J=2$ would result in each tree being a stump, and no variable-variable interaction. $J=3$ would allow for a maximum of 2 variables per tree, and a max of 3 leaves. This is a strong controller of the complexity of the trees in the algorithm and coupled with the learning rate can have a big effect on computation time.


